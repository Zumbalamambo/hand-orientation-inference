
\subsection*{Hand Pose and Orientation Inference}
Building discriminative models for hand pose and orientation inference is a task which typically requires non-standard architectures due to its intricacy. Global and local features are both useful for these tasks due to the intricacy in modelling a hand. Training capacity must be used to model very small changes in a hand and more global features are often sufficient to dismiss many poses or orientations.\\

This technique is used in [13] where a multiresolution CNN is used on a dataset of RGBD images for real time pose-recovery. A 96x96 image is downsampled 2x and 4x, where the more downsampled an image the more global the features which can be extracted. These images are passed into separate CNNs before their feature maps are flattened, concatenated and passed into fully-connected layers. It would of course be an alternative to use larger filters instead of downsampling but this increases learning capacity greatly and makes the model less likely to generalise.\\

In [13] the network is trained to generate 14 18x18 heatmaps which represent the probability of a given joint being in a location. These heatmaps are then combined using the inverse kinematics algorithm to recover the pose of the hand. Note that the training is compartmentalised into 14 separate heatmaps each representing a separate joint, rather than learning the location of all joints individually. These intermediate heatmaps allow the network to focus on local features which increases accuracy. It also allows better recovery of pose if a heatmap fails as the other non-failing heatmaps can be used with a heuristic.\\

[13] is not unique in using joint angle and position for solving hand pose estimation. [14], for example directly estimates 3D joint location with a variety of architectures including, multi-scale, deep and shallow. In [14], as well, the physical constraints of joint positions in a hand are also used in alternate architecture which models first a low dimensional embedding. This lower implicit dimensionality forces the network to learn whilst the physical constraints of the hands are enforced. There is an additional reconstruction layer which projects the lower dimensional vector back into higher dimensional joint space. In this way, it still models the joint locations directly, but the low dimensional embedding before the reprojection acts as a 'bottleneck', restricting training. [14] goes further in a refining stage where a separate network is used for each joint along with a system which corrects overlapping regions.\\

[15], by the same authors as in [14], again estimates 3D joint location but uses these locations to develop synthesised images of hands. Synthesised images of hands using poorly estimated 3D joint location will result in an image which does not well represent a hand and will be malformed in shape. For refinement, something similar to a Siamese network is then used in [15]. This network is a multi-input network with a weight constraint in each path such that both paths have identical weights before their output is concatenated before being passed into shared fully-connected layer.\\

This network has the synthesised image and the true image passed into the Siamese network architecture which iteratively updates the 3D joint location to reduce disparity between the synthesised image and the true image which leads to more accurate 3D joint location estimation. To increase the speed of this convergence a given factor between 0 and 1 is used as a minimal improvement along with Gaussian noise being applied to the poses which allows the syntesised image to converge more quickly to the true image.\\

[16] uses fully 3D information, noting that in [13] the joint location probability heat maps which are the results of the regression only contain 2D information. This is done by modelling on multiple views which are obtained by projecting the original single-depth images onto three orthogonal planes. Because the 2D positions of the joints are projected onto multiple planes, multiple-view CNNs are then more robust to errors in 2D location estimation which could results in large depth estimation errors in single-view CNNs. They are also more robust to ambiguity and can learn hand constraints implicitly as opposed to it being pre-defined as in [13]. This technique was able to achieve state-of-the-art results on multiple datasets including ones which originally relied on calibration and tracking rather than being purely data-driven.\\

This work will focus exclusively on hand orientation on 2D uncalibrated monocular images and will follow on from work in [17] where azimuth and elevation axis angles are modelled on directly. [17] has a focus on developing a generalizable technique in which hand orientation can be inferred from multiple viewpoints on any hand in a quick manner, that is one which does not require calibration.\\

[17]'s best results are achieved using a ensemble of expert regressors which each focus on a particular area of angle space.  There is a first layer which outputs the posterior probality of a given example belonging to a particular angle space. This of course allows the model to dismiss large regions of angle space and gain benefit from the expert regressors. Note that all of the regressors in [17] are Random Forests which use contour distance features as input. Deep convolutional networks are then shown not to be necessary for this specific task.\\

[17] develops a method in which each expert regressor will output a posterior probability of an example belonging to its subspace. However, instead of using these posterior probabilities naively, during training the ground truth probabilities are used alongside the expert regressor's posterior probabilities to learn a marginalised probability distribution from which the marginalised regressors weights are derived from. Essentially, this is learning a way to combine the  posterior probabilites to form a marginalised probability distribution most similar to the ground truth probability. This was done using a novel optimisation technique similar to  Kullback-Leibler divergence***.\\

[17] using a marginalisation layer followed by expert regressors would make their approach a multi-layer Random Forest (ML-RF hereafter. Whereas [17] developed a novel and generalizable technique as previously described, a significant body of work exists also utilising ML-RF for similar tasks. [18] uses an ML-RF but for hand-pose estimation. The first layer classifies a general shape and the second layer focuses on classifying particular parts of the hand. [18] most noticeably differs from [17] in that the posterior probabilities are used naively and are simply used to weight the posterior probabilities of the second layer. They are not used to model a marginal probability during training. \\

ML-RF techniques are generally reliant on the results of the first layer. [19] is an example of this as it uses cascaded regressors, where each following regressor depends on the results of the previous. This technique was motivated by relationships between different parts of the hand, as certain impossibilities in shape allow a hierarchical logic to define how the cascaded regressors depend on each other. However, it is impossible to recover from weak results in the first layer. In [17] and [18] marginalisation and weighted sums are used which can buffer any inaccuracies but not avoid them. \\

ML-RF techniques are computationally cheap and able to achievement acceptable performance but Random Forests are not able to build complex combined features as CNNs are able to. These features allow much greater accuracy and can model on the hand directly whereaas Random Forests must rely on extracted features as well. CNNs consistently achieve state-of-the-art on this task undoubtedly due to these advantages. Vanilla CNNs are extremely costly computationally both during training and run-time. However, recent advances have drastically reduced the cost and size of CNNs with minimal to no loss in computational accuracy. This makes them potentially suitable for this task in real-time. How computaional cost is decreased is described in the following section. \\

It is important to note that despite complex CNN architectures achieving state of the art performance in many hand pose or orientation estimation tasts, the gains are not significant over Random Forest methods. A reason noted in [20] is that the CNNs architectures than have emerged are relatively shallow compared to those where achieve state of the art on other tasks such as ImageNet ***. This is partly due to avoid overfitting on small datasets such as the one that will be used in this task. \\

(unlabelled comment? can they use unlabelled in gan no?)
Small datasets are common in hand pose and orientation estimation tasks as they typically rely on manual annotation.   Generic dataset augmentation techniques, such as translation and rotation of images are still viable for hand pose estimation but can only generate a limited amount of additional data. 

Recent advances in generative adversarial techniques have allowed synthetic images to become realistic enough to be used for training [21]. Generative adversarial networks originally proposed in [29] utilise two networks, a generative one and a discriminator. The generative network will generate data for which the discriminator outputs a probability of it having appeared in the original training data. The generative network gradually improves until it can generate data that is indiscernible from the original training data according to the discriminator. \\

Generative adversarial data augmentation techniques can produce a much wider range of augmentations than standard techniques. However, images generated by adversarial networks can often have large artefacts and distortions. Before recent developments [21, 30] synthetic images were not realistic enough and models could not generalize when used on real images.SimGAN [21] uses a refinement stage to avoid artifacts and generate images more accurately such that they are suitable for use as training data. This is done using self-regularization and local adversarial loss which use smaller receptive fields which help avoid drift and create more life-like images.\\\\

Using these techniques large improvements were achieved on several eye direction and hand pose datasets using the refined syntheitcs images compared to the original datasets. MPIIGaze showed a 22.3\% improvement in accuracy using the refined synthetic images compared to the original dataset. [21] does not implement the customized pipelines that achieve state-of-the-art for NYU Hand Pose dataset, just a vanilla CNN, but are able to achieve 8.8\% improvement in accuracy using the refined synthetic images.\\

 
is this a copy? add comment on why you are not using this i.e. because it is just well documented gain. 
SimGAN, the technique developed in [21] generates images adversarially in the first stage until the images fool the adversarial network and then they are refined  in a refiner network. Self-regularization and local adversarial loss enable much more accurate or life-like as the smaller receptive fields help avoid drift and prevent artifacts form occuring. State-of-the-art is achieved on MPIIGaze with 22.3\% improvement in accuracy using the refined synthetic images compared to the original dataset. [21] do not implement the customized pipelines that achieve state-of-the-art for NYU Hand Pose dataset, just a vanilla CNN, but are able to achieve 8.8\% improvement in accuracy using the refined synthetic images.\\

These results are not unique to hand pose and eye direction datasets and were recently shown in [30] to achieve between 2-14\% on a variety of benchmark datasets. Their architecture, called DAGAN presents both the true image and the images that are generated from it to the discriminator. In this way, the adversarial network can be encouraged to produce images that vary from the true image and does not simply try to create a replica behaving like an autoencoder (VAE).\\

Further work generative adversarial data augmentation specifically for hand modelling exsits in which [25] a VAE is used to model a lower-dimension representation of 3D hand pose. This has the effect of allowing the discriminator to be on trained on examples which can pass through the generative process any given number of times. The lower-dimension represetnations are used by a GAN to develops a realistic depth map which is what the discriminator is trained on jointly. 

Note that the generative techniques in [21, 25] are for dataset generation or augmentation and remain completely separate from previous work which uses generative techniques to infer hand pose directly. These more typical modern generative techniques are largely based around building a 3D model of the hand and tracking the individual components of the model in real-time in order to infer the pose. This was done  in [26] where a hand model  where the hand model was build using geometric primitives.\\

These generative techniques typically rely on strong computational power with multi-GPU set ups and typically fail to generalize on a new user's hand which cause them to be completely separate to what is aiming to be achieved in this work. Generative techniques also suffer from drift, that is accumulation of errors, which was confronted in [21] using local adversarial loss.\\ 

In [20], several best practices are explored whilst using an Regional Ensemble Net architecure, which similarly to other. architectures models directly on 3D hand joint co-ordinates. The Regional Ensemble Net architecture produces feature maps which focus on different regions of the image. These region wise feature maps are concatenated and passed into fully-connected layers where they can combine into complex joint features. Concatenating these feature maps rather than using average pooling across the feature maps was shown to improve performance again as the network can learn itself how best to combine them. [20] is unique in that different views of inputs are used simultaneously during training to predict the same pose, rather than just during testing to check robustness. \\

The best practices shown to improve performance in [20] were data augmentation, smooth L1 loss and patch cropping, where a cube from the center of the depth image is extracted and used as further input for training. * add section 2 here

\subsection*{Memory Overhead Management}
Traditional CNNs, despite their ability in achieving high accuracy on many tasts, have several computational inefficiencies that are more apparent when developing large models or relying on low resource hardware. These inefficiencies  arise both from their spatial convolutions and CNNs typical reliance on many full-connected layers at the top of the network which often contain the vast majority of the parameters. In developing efficient networks, both in terms of latency and size, it is possible to work around these inefficiencies in many ways.\\

MobileNets [1] utilize depthwise separable convolutions which are efficient both in terms of latency  and number of parameters. These are factorized convolutions in wich depthwise convolutions are performed separately on each input channel. These are then followed by 1x1 pointwise convolutions on each feature map and only then are outputs combined.\\ 

By separating these stages computational cost is reduced 8-9x when using 3x3 depthwise separable convolutions as in [1]. [2] notes that separating depthwise and pointwise convolutions also prevents a single convolutional kernel having to map spatial correlations and cross-channel correlations jointly. Depthwise separable convolutions are utilised heavily in very deep architectures focused on achieving state-of-the-art results [2, 10]. MobileNets [1], however, utilise them for their efficiency. \\

MobileNets have a base structure which is then controlled by model-shrinking parameters. One parameter alters the width of each layer in the network uniformly and the other is a resolution multiplier. Note that both of these parameters are only used to parametrise a new structure which must be trained again. They primarily exist to provide ease of use for developers aiming to optimise the cost of their network.\\

The base structure for a MobileNet is a layer of full convolutions followed by depthwise separable convolutions all with ReLU activation functions. Batch normalisation and strided convolution downsampling is performed between each layer. Average pooling is then performed before output is passed to the fully-connected layers.\\

The utilisation of 1x1 convolutions in MobileNets is not unique to that architecture and are generally popular in model architectures motivated by low latency and model size. SqueezeNets [8] use them not only for the fewer parameters but also to reduce the number of input channels. This is done in Fire modules which replace the convolutional layers. Fire modules have a squeeze layers within them which utilise 1x1 convolutions to greatly restrict the number of input channels being pased to the expand layer.. Fire modules are then much more parameter efficient than standard convolutional layers and are described in more detail in the model section***. SqueezeNets also delay downsampling until later in the network rather than having to downsample each layer with pooling which is shown to increase classification accuracy in [9].\\

SqueezeNets are in fact fully convolutional which further reduces parameters by avoiding the large number of weights which fully-connected layers introduce. Despite this, the model size can still be reduced \emph{considerably} using Deep Compression  [11] and still not lose classification accuracy.\\

Deep Compression is a procedure of several stages which allows compression of models 35-50x without loss of accuracy. This allows extremely large models to still fit on embedded devices. Although the focus is largely on model size, latency can be lowered if the model is reduced enough in size to avoid having to store it in off-chip DRAM memory [11]. Note that models which are naturally parameter efficient such as SqueezeNets also are able to gain the benefits of small model size. It is also noted in [9] that this generally allows models to be stored on-chip on FPGAs which are extremely low resource with only 10MB  memory. This is necessary for real time applications. \\

Pruning is utilised in [11] which is when weights below a certain threshold are simply discarded from the network. This sparse structure is then efficiently stored in compressed sparse row or compressed sparse column format. The weights are then split into clusters which allows the weight matrix to again expressed in a further compressed form as only a table of shared weights needs to be stored, not the individual ones. Finally, Huffman coding is applied to the quantized weights.\\

Other model compression techniques focus solely on exploiting natural redundancies that occur in fully-connected weight matrices. In [3], which is very similar in architecture and task to this work, over 80\% of the parameters lay in the fully-connected layers but fully-connected layers can of course make up an even larger share of parameters.\\

One technique in exploiting these redunancies is Tensor-Train (TT hereafter) decomposition [4] which was used in [5] in training deep neural networks. In [5] fully-connected layers are replaced with TT-layers which means the weight matrix is expressed in TT-format. A d-dimensional tensor is represented in TT-format if each of its elements can be computed using a product of matrices each representing a dimension. Note that multiple matrices can represent one dimension. These groups of matrices are called cores and a tensor in TT-format can equivalently be expressed as sum of the product of these cores. \\

TT-decompositions of tensors into TT-format greatly reduce the number of parameters as the TT-rank can be specified to be low. The TT-rank is equivalent to how many cores are present in the TT-decomposition. Any tensor can be represented in TT-format with a sufficiently high TT-rank.\\

Tensors in TT-format can still have linear algebra operations applied to them which makes training simpler. This is useful as the naive method of training is very costly. It would involve using stochastic gradient descent on the weight matrix directly and then converting the weight matrix to TT-format with a singular value decomposition algorithm such as the TT-SVD algorithm. With many parameters this can be costly at O(MN) but if the loss function gradient is computed with respect to the cores directly memory cost can be reduced to O(d2 r4 max{M, N}) which is another attractive feature of the TT-layer.[5]\\

TT-rank times Oseledets \& Tyrtyshnikov, 2009\\

Another technique to exploit the natural redundancies in weight matrices of fully-connected layers is to hash the weight connections into buckets which will then share parameters during train. This is the technique used in HashedNets [7] which are very effective in compressing network size. That is the test error of networks is much more stable as the compression factor increased as compared to other compression techniques whose test errors are shown to diverge wildly as the compression factor is more than 8x. The test error for HashedNets is only marginally worse than full size even with a compression factor of 64x. [7]\\

Note this compression factor does not improve latency as it is just an efficient way of expressing a large network. The same amount of computation is needed but more weights are shared. This compression factor is useful for very large networks which could consume GPU memory during training [7] but is less relevant to being able to achieve real-time performance on low resource hardware. 

\subsection*{Learning-Based Network Compression}
Other compression techniques are learning based and are separate for the more architectural compression techniques previously described. These techniques are generally referred to as knowledge distillation [12]. This technique works by building an objective function based on softmax layer output of a larger network, or several networks, as training labels. The correct labels can also be used in a second objective function but in [12] it was shown that the best accuracy is gained if a very low weight is put on this objective function.\\

The softmax distribution is parametrised using temperature as follows:

\[qi = \frac{exp(z_i/T)}{\sum_j exp(z_j/T)}\]

Typically a high temperature is used which causes a softer distribution where the probabilities are spread over more classes. This is where the benefit of knowledge lies as the softmax layer can store information about similarity between classes that would not be stored in a simple one-hot layer.\\

In [12] this technique is also scaled using ensembles of specialist models on Google's JFT dataset. This dataset has over 15,000 labels showing very clear benefits of training specialist models. In this work, where the specialist models simply train on a specific part of angle-space, there are minimal benefits to training. The labels of the larger, specialised models will just be less accurate than the true labels and do not hold extra information to distill into the smaller networks. Even training on the discriminator model has little benefit as the similarity between classes is known as they are related in known angle-space.

\subsection{Extra}
ML-RF techniques are generally reliant on the results of the first layer. [19] is an example of this as it uses cascaded regressors, where each following regressor depends on the results of the previous. This technique was motivated by relationships between different parts of the hand, as certain impossibilities in shape allow a hierarchical logic to define how the cascaded regressors depend on each other. However, it is impossible to recover from weak results in the first layer. In [17] and [18] marginalisation and weighted sums are used which can buffer any inaccuracies but not avoid them. \\


was in methods
The performance both in terms of latency and accuracy will be compared between SqueezeNets, MobileNets and Vanilla CNNs. I define a Vanilla CNN as one utilising 2D spatial convolutions followed by fully-connected layers. SqueezeNets were chosen as they do not utilise fully-connected layers. In this way, it gives an opportunity to use significantly fewer parameters than the other models.\\

 pruning
***Notes
number of mult-adds not equal to performance i.e. irregular sparsity creates bottlenecks and it doesn't help. so random pruning helps but is largely for model size. less reg, less data aug -> small models don't have problems with overfitting as much.

Note that no network compression techniques will be experimented with. This decision was made as networks for hand-orientation or hand-pose estimation tasks tend to be fairly shallow. I deemed it to be more beneficial to focus on latency alone and SqueezeNets provide a very low latency option due to the lack of fully-connected layers.\\

Also, it is a well known result that memory compression techniques can reduce model size without lowering performance drastically. As the focus of this work is not merely to train the lowest possible latency model, but also to generally test convolutional methods on a dataset which has previously only had ML-RF techniques performed it, it is not beneficial to focus too much on model compression and showing a well known results again. That is I prefer to experiment with CNNs designed for low latency rather than generic model compression techniques.\\