\documentclass{article}
\usepackage{enumitem}
\begin{document}
\section*{Multiresolution Convolutional Neural Network Architectures for Hand Orientation Inference}

\subsection*{Introduction}
In this work an efficient convolutional neural network (CNN) architecture will be developed which is capable of inferring hand orientation through regression on uncalibrated 2D monocular images. This is a useful technique in augumented reality and human computer interaction which often rely on low resource hardware such as handheld mobile devices.\\

Many challenges have plagued this task due to the complicated nature of hands. For instance natural self occlusion, high joint flexibility and the necessity to be able to model from a wide variety of view points for approaches to be useful in real life situations. \\

\subsection*{Hand Pose and Orientation Inference}
Building discriminative models for hand pose and orientation inference is a task which typically requires non-standard architectures due to its intricacy. Global and local features are both useful for these tasks due to the intricacy in modelling a hand. Training capacity must be used to model very small changes in a hand and more global features are often sufficient to dismiss many poses or orientations.\\

This technique is used in [13] where a multiresolution CNN is used on a dataset of RGBD images for real time pose-recovery. A 96x96 image is downsampled 2x and 4x, where the more downsampled an image the more global the features which can be extracted. These images are passed into separate CNNs before their feature maps are flattened, concatenated and passed into fully-connected layers. It would of course be an alternative to use larger filters instead of downsampling but this increases learning capacity greatly and makes the model less likely to generalise.\\

In [13] the network is trained to generate 14 18x18 heatmaps which represent the probability of a given joint being in a location. These heatmaps are then combined using the inverse kinematics algorithm to recover the pose of the hand. Note that the training is compartmentalised into 14 separate heatmaps each representing a separate joint, rather than learning the location of all joints individually. These intermediate heatmaps allow the network to focus on local features which increases accuracy. It also allows better recovery of pose if a heatmap fails as the other non-failing heatmaps can be used with a heuristic.\\

[13] is not unique in using joint angle and position for solving hand pose estimation. [14], for example directly estimates 3D joint location with a variety of architectures including, multi-scale, deep and shallow. In [14], as well, the physical constraints of joint positions in a hand are also used in alternate architecture which models first a low dimensional embedding. This lower implicit dimensionality forces the network to learn whilst the physical constraints of the hands are enforced. There is an additional reconstruction layer which projects the lower dimensional vector back into higher dimensional joint space. In this way, it still models the joint locations directly, but the low dimensional embedding before the reprojection acts as a 'bottleneck', restricting training. [14] goes further in a refining stage where a separate network is used for each joint along with a system which corrects overlapping regions.\\

[15], by the same authors as in [14], again estimates 3D joint location but uses these locations to develop synthesised images of hands. Synthesised images of hands using poorly estimated 3D joint location will result in an image which does not well represent a hand and will be malformed in shape. For refinement, something similar to a Siamese network is then used in [15]. This network is a multi-input network with a weight constraint in each path such that both paths have identical weights before their output is concatenated before being passed into shared fully-connected layer.\\

This network has the synthesised image and the true image passed into the Siamese network architecture which iteratively updates the 3D joint location to reduce disparity between the synthesised image and the true image which leads to more accurate 3D joint location estimation. To increase the speed of this convergence a given factor between 0 and 1 is used as a minimal improvement along with Gaussian noise being applied to the poses which allows the syntesised image to converge more quickly to the true image.\\

[16] uses fully 3D information, noting that in [13] the joint location probability heat maps which are the results of the regression only contain 2D information. This is done by modelling on multiple views which are obtained by projecting the original single-depth images onto three orthogonal planes. Because the 2D positions of the joints are projected onto multiple planes, multiple-view CNNs are then more robust to errors in 2D location estimation which could results in large depth estimation errors in single-view CNNs. They are also more robust to ambiguity and can learn hand constraints implicitly as opposed to it being pre-defined as in [13]. This technique was able to achieve state-of-the-art results on multiple datasets including ones which originally relied on calibration and tracking rather than being purely data-driven.\\

This work will focus exclusively on hand orientation on 2D uncalibrated monocular images and will follow on from work in [17] where azimuth and elevation axis angles are modelled on directly. [17] has a focus on developing a generalizable technique in which hand orientation can be inferred from multiple viewpoints on any hand in a quick manner, that is one which does not require calibration.\\

[17]'s best results are achieved using a ensemble of expert regressors which each focus on a particular area of angle space.  There is a first layer which outputs the posterior probality of a given example belonging to a particular angle space. This of course allows the model to dismiss large regions of angle space and gain benefit from the expert regressors. Note that all of the regressors in [17] are Random Forests which use contour distance features as input. Deep convolutional networks are then shown not to be necessary for this specific task.\\

[17] develops a method in which each expert regressor will output a posterior probability of an example belonging to its subspace. However, instead of using these posterior probabilities naively, during training the ground truth probabilities are used alongside the expert regressor's posterior probabilities to learn a marginalised probability distribution from which the marginalised regressors weights are derived from. Essentially, this is learning a way to combine the  posterior probabilites to form a marginalised probability distribution most similar to the ground truth probability. This was done using a novel optimisation technique similar to  Kullback-Leibler divergence***.\\

[17] using a marginalisation layer followed by expert regressors would make their approach a multi-layer Random Forest (ML-RF hereafter. Whereas [17] developed a novel and generalizable technique as previously described, a significant body of work exists also utilising ML-RF for similar tasks. [18] uses an ML-RF but for hand-pose estimation. The first layer classifies a general shape and the second layer focuses on classifying particular parts of the hand. [18] most noticeably differs from [17] in that the posterior probabilities are used naively and are simply used to weight the posterior probabilities of the second layer. They are not used to model a marginal probability during training. \\

ML-RF techniques are generally reliant on the results of the first layer. [19] is an example of this as it uses cascaded regressors, where each following regressor depends on the results of the previous. This technique was motivated by relationships between different parts of the hand, as certain impossibilities in shape allow a hierarchical logic to define how the cascaded regressors depend on each other. However, it is impossible to recover from weak results in the first layer. In [17] and [18] marginalisation and weighted sums are used which can buffer any inaccuracies but not avoid them. \\

ML-RF techniques are computationally cheap and able to achievement acceptable performance but Random Forests are not able to build complex combined features as CNNs are able to. These features allow much greater accuracy and can model on the hand directly whereaas Random Forests must rely on extracted features as well. CNNs consistently achieve state-of-the-art on this task undoubtedly due to these advantages. Vanilla CNNs are extremely costly computationally both during training and run-time. However, recent advances have drastically reduced the cost and size of CNNs with minimal to no loss in computational accuracy. This makes them potentially suitable for this task in real-time. How computaional cost is decreased is described in the following section. \\

It is important to note that despite complex CNN architectures achieving state of the art performance in many hand pose or orientation estimation tasts, the gains are not significant over Random Forest methods. A reason noted in [20] is that the CNNs architectures than have emerged are relatively shallow compared to those where achieve state of the art on other tasks such as ImageNet ***. This is partly due to avoid overfitting on small datasets such as the one that will be used in this task. \\

Recent advances in generative adversarial techniques have allowed synthetic images to become realistic enough to be used for training [21]. This is desirable as it avoids the difficult on training on small datasets which require manual annotation or  Kinect to for labelling. This means many synthetic images can be used to provide multiple examples for any one given annotation. Before these developments synthetic images were not realistic enough and models could not generalize when used on real images.\\

SimGAN, the technique developed in [21] generates images adversarially in the first stage until the images fool the adversarial network and then they are refined  in a refiner network. Self-regularization and local adversarial loss enable much more accurate or life-like as the smaller receptive fields help avoid drift and prevent artifacts form occuring. State-of-the-art is achieved on MPIIGaze with 22.3\% improvement in accuracy using the refined synthetic images compared to the original dataset. [21] do not implement the customized pipelines that achieve state-of-the-art for NYU Hand Pose dataset, just a vanilla CNN, but are able to achieve 8.8\% improvement in accuracy using the refined synthetic images.\\

Note that the generative techniques in [21] are for dataset generation and remain completely separate from previous work which uses generative techniques to infer hand pose directly. These generative techniques typically rely on strong computational power with multi-GPU set ups and typically fail to generalize on a new user's hand which cause them to be completely separate to what is aiming to be achieved in this work. Generative techniques can not be dismissed completely and are capable of achieving extremely accurate results as in [25] which takes advantage of adversarial loss *** \\

In [20], several best practices are explored whilst using an Regional Ensemble Net architecure, which similarly to other architectures models directly on 3D hand joint co-ordinates. The Regional Ensemble Net architecture produces feature maps which focus on different regions of the image. These region wise feature maps are concatenated and passed into fully-connected layers where they can combine into complex joint features. Concatenating these feature maps rather than using average pooling across the feature maps was shown to improve performance again as the network can learn itself how best to combine them. [20] is unique in that different views of inputs are used simultaneously during training to predict the same pose, rather than just during testing to check robustness. \\

The best practices shown to improve performance in [20] were data augmentation, smooth L1 loss and patch cropping, where a cube from the center of the depth image is extracted and used as further input for training. * add section 2 here

\subsection*{Memory Overhead Management}
Traditional CNNs, despite their ability in achieving high accuracy on many tasts, have several computational inefficiencies that are more apparent when developing large models or relying on low resource hardware. These inefficiencies  arise both from their spatial convolutions and CNNs typical reliance on many full-connected layers at the top of the network which often contain the vast majority of the parameters. In developing efficient networks, both in terms of latency and size, it is possible to work around these inefficiencies in many ways.\\

MobileNets [1] utilize depthwise separable convolutions which are efficient both in terms of latency  and number of parameters. These are factorized convolutions in wich depthwise convolutions are performed separately on each input channel. These are then followed by 1x1 pointwise convolutions on each feature map and only then are outputs combined.\\ 

By separating these stages computational cost is reduced 8-9x when using 3x3 depthwise separable convolutions as in [1]. [2] notes that separating depthwise and pointwise convolutions also prevents a single convolutional kernel having to map spatial correlations and cross-channel correlations jointly. Depthwise separable convolutions are utilised heavily in very deep architectures focused on achieving state-of-the-art results [2, 10]. MobileNets [1], however, utilise them for their efficiency. \\

MobileNets have a base structure which is then controlled by model-shrinking parameters. One parameter alters the width of each layer in the network uniformly and the other is a resolution multiplier. Note that both of these parameters are only used to parametrise a new structure which must be trained again. They primarily exist to provide ease of use for developers aiming to optimise the cost of their network.\\

The base structure for a MobileNet is a layer of full convolutions followed by depthwise separable convolutions all with ReLU activation functions. Batch normalisation and strided convolution downsampling is performed between each layer. Average pooling is then performed before output is passed to the fully-connected layers.\\

The utilisation of 1x1 convolutions in MobileNets is not unique to that architecture and are generally popular in model architectures motivated by low latency and model size. SqueezeNets [8] use them not only for the fewer parameters but also to reduce the number of input channels. This is done in Fire modules which replace the convolutional layers. Fire modules have a squeeze layer which consists solely of 1x1 filters, these immediately limit the number of input channels which are passed to the expand layer which consists of 1x1 and 3x3 filters. Fire modules are then much more parameter efficient than standard convolutional layers. This allows SqueezeNets to delay downsampling until later in the network rather than having to downsample each layer with pooling which is shown to increase classification accuracy in [9].\\

SqueezeNets are in fact fully convolutional which further reduces parameters by avoiding the large number of weights which fully-connected layers introduce. Despite this, the model size can still be reduced \emph{considerably} using Deep Compression  [11] and still not lose classification accuracy.\\

Deep Compression is a procedure of several stages which allows compression of models 35-50x without loss of accuracy. This allows extremely large models to still fit on embedded devices. Although the focus is largely on model size, latency can be lowered if the model is reduced enough in size to avoid having to store it in off-chop DRAM memory [11].\\

Pruning is utilised in [11] which is when weights below a certain threshold are simply discarded from the network. This sparse structure is then efficiently stored in compressed sparse row or compressed sparse column format. The weights are then split into clusters which allows the weight matrix to again expressed in a further compressed form as only a table of shared weights needs to be stored, not the individual ones. Finally, Huffman coding is applied to the quantized weights.\\

Other model compression techniques focus solely on exploiting natural redundancies that occur in fully-connected weight matrices. In [3], which is very similar in architecture and task to this work, over 80\% of the parameters lay in the fully-connected layers but fully-connected layers can of course make up an even larger share of parameters.\\

One technique in exploiting these redunancies is Tensor-Train (TT hereafter) decomposition [4] which was used in [5] in training deep neural networks. In [5] fully-connected layers are replaced with TT-layers which means the weight matrix is expressed in TT-format. A d-dimensional tensor is represented in TT-format if each of its elements can be computed using a product of matrices each representing a dimension. Note that multiple matrices can represent one dimension. These groups of matrices are called cores and a tensor in TT-format can equivalently be expressed as sum of the product of these cores. \\

TT-decompositions of tensors into TT-format greatly reduce the number of parameters as the TT-rank can be specified to be low. The TT-rank is equivalent to how many cores are present in the TT-decomposition. Any tensor can be represented in TT-format with a sufficiently high TT-rank.\\

Tensors in TT-format can still have linear algebra operations applied to them which makes training simpler. This is useful as the naive method of training is very costly. It would involve using stochastic gradient descent on the weight matrix directly and then converting the weight matrix to TT-format with a singular value decomposition algorithm such as the TT-SVD algorithm. With many parameters this can be costly at O(MN) but if the loss function gradient is computed with respect to the cores directly memory cost can be reduced to O(d2 r4 max{M, N}) which is another attractive feature of the TT-layer.[5]\\

TT-rank times Oseledets \& Tyrtyshnikov, 2009\\

Another technique to exploit the natural redundancies in weight matrices of fully-connected layers is to hash the weight connections into buckets which will then share parameters during train. This is the technique used in HashedNets [7] which are very effective in compressing network size. That is the test error of networks is much more stable as the compression factor increased as compared to other compression techniques whose test errors are shown to diverge wildly as the compression factor is more than 8x. The test error for HashedNets is only marginally worse than full size even with a compression factor of 64x. [7]\\

Note this compression factor does not improve latency as it is just an efficient way of expressing a large network. The same amount of computation is needed but more weights are shared. This compression factor is useful for very large networks which could consume GPU memory during training [7] but is less relevant to being able to achieve real-time performance on low resource hardware. 

\subsection*{Learning-Based Network Compression}
Other compression techniques are learning based and are separate for the more architectural compression techniques previously described. These techniques are generally referred to as knowledge distillation [12]. This technique works by building an objective function based on softmax layer output of a larger network, or several networks, as training labels. The correct labels can also be used in a second objective function but in [12] it was shown that the best accuracy is gained if a very low weight is put on this objective function.\\

The softmax distribution is parametrised using temperature as follows:

\[qi = \frac{exp(z_i/T)}{\sum_j exp(z_j/T)}\]

Typically a high temperature is used which causes a softer distribution where the probabilities are spread over more classes. This is where the benefit of knowledge lies as the softmax layer can store information about similarity between classes that would not be stored in a simple one-hot layer.\\

In [12] this technique is also scaled using ensembles of specialist models on Google's JFT dataset. This dataset has over 15,000 labels showing very clear benefits of training specialist models. In this work, where the specialist models simply train on a specific part of angle-space, there are minimal benefits to training. The labels of the larger, specialised models will just be less accurate than the true labels and do not hold extra information to distill into the smaller networks. Even training on the discriminator model has little benefit as the similarity between classes is known as they are related in known angle-space.

\subsection*{Network Architecture}
To avoid saturation of gradients in the network, rectified activation units will be used. Standard rectified linear units (ReLU hereafter) reassign all negative values to zero and lets positive values pass unchanged. This makes ReLUs computationally very efficient compared to parametric non-linear activation functions such as a sigmoidal. However, as all negative values are set to 0, large updates can cause gradient not to be able to pass through the ReLU.\\

This problem can cause large parts of the network to die and results in relatively sparse network. This sparsity was often attributed to the state-of-the-art performances of ReLU. However, advances in rectified activation unit design brought forward units which allow negative gradients to pass. An example is LeakyReLU which scales negative values with a large denominator however this denominator can be parametrised to let more gradient through. Randomized leaky rectified units (RRelu) randomize the amount of negative gradient that passes through and in [6] outperformed other rectified activation units due to the additional regularization.\\

[6] also points out that LeakyReLU units which were very leaky, as in they let more gradient through, outperformed less leaky ones which are more common. It is important to note, however, that the difference in performance between different rectified activation units in [6] is very marginal.\\

Also, using a fairly complex network structure on an intricate task with a fairly small dataset, particular attention must be paid to the initialization strategy of the network weights. The efficacy of weight intialization and its ability to allow convergence of deep networks was brough to the forefront in [22] with Xavier intialization. For ReLUs, however, the optimal weight strategy of giving weights variance of $1/N$ (where N is the number of units in a layer) was shown not to hold true. Specifically, in [24] it is shown that the optimal variance weights in ReLU based networks is in fact $2/N$ and in many cases networks fail to converge under Xavier intialization. This is not merely empirically observed but the result is rigorously proved in [23].\\

\subsection*{References}
\begin{enumerate}
\item Howard, A. G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., ... \& Adam, H. (2017). MobileNets: Efficient convolutional neural networks for mobile vision applications. \emph{arXiv preprint arXiv:1704.04861}.
\item Chollet, F. (2016). Xception: Deep Learning with Depthwise Separable Convolutions. \emph{arXiv preprint arXiv:1610.02357}.
Chicago	
\item Tompson, J., Stein, M., Lecun, Y. \& Perlin, K., 2014. Real-time continuous pose recovery of human hands using convolutional networks. \emph{ACM Transactions on Graphics (ToG)},33(5), p.169.
\item Oseledets, I. V. (2011). Tensor-train decomposition. \emph{SIAM Journal on Scientific Computing}, 33(5), 2295-2317.
Chicago	
\item Novikov, A., Podoprikhin, D., Osokin, A., \& Vetrov, D. P. (2015). Tensorizing neural networks. \emph{Advances in Neural Information Processing Systems} (pp. 442-450).
\item Xu, B., Wang, N., Chen, T., \& Li, M. (2015). Empirical evaluation of rectified activations in convolutional network. \emph{arXiv preprint arXiv:1505.00853}.
\item Chen, W., Wilson, J., Tyree, S., Weinberger, K., \& Chen, Y. (2015, June). Compressing neural networks with the hashing trick. \emph{International Conference on Machine Learning} (pp. 2285-2294).
\item Iandola, F. N., Han, S., Moskewicz, M. W., Ashraf, K., Dally, W. J., \& Keutzer, K. (2016). SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and< 0.5 MB model size. \emph{arXiv preprint arXiv:1602.07360}.
\item He, K., \& Sun, J. (2015). Convolutional neural networks at constrained time cost. \emph{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition} (pp. 5353-5360).
\item Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., \& Wojna, Z. (2016). Rethinking the inception architecture for computer vision. \emph{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition} (pp. 2818-2826).
\item S. Han, H. Mao, \& W. Dally. Deep compression: Compressing DNNs with pruning, trained
quantization and huffman coding. \emph{arxiv:1510.00149v3, 2015a}.
\item Hinton, Geoffrey, Oriol Vinyals, \& Jeff Dean.  (2015). Distilling the knowledge in a neural network. \emph{arXiv preprint arXiv:1503.02531}.
\item Tompson, J., Stein, M., Lecun, Y., \& Perlin, K. (2014). Real-time continuous pose recovery of human hands using convolutional networks. \emph{ACM Transactions on Graphics (ToG)}, 33(5), 169.
\item Oberweger, M., Wohlhart, P., \& Lepetit, V. (2015). Hands deep in deep learning for hand pose estimation. \emph{arXiv preprint arXiv:1502.06807}.
\item Oberweger, M., Wohlhart, P., \& Lepetit, V. (2015). Training a feedback loop for hand pose estimation. In \emph{Proceedings of the IEEE International Conference on Computer Vision} (pp. 3316-3324).
\item Ge, L., Liang, H., Yuan, J., \& Thalmann, D. (2016). Robust 3D hand pose estimation in single depth images: from single-view CNN to multi-view CNNs. \emph{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition} (pp. 3593-3601).
\item Asad, M. (2017). Efficient hand orientation and pose estimation for uncalibrated cameras \emph{(Doctoral dissertation, City, University of London)}.
\item Keskin, C., Kıraç, F., Kara, Y. E., \& Akarun, L. (2012, October). Hand pose estimation and hand shape classification using multi-layered randomized decision forests. In \emph{European Conference on Computer Vision (pp. 852-863). Springer Berlin Heidelberg}.
\item Sun, X., Wei, Y., Liang, S., Tang, X., \& Sun, J. (2015). Cascaded hand pose regression. In \emph{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition} (pp. 824-832).
\item Guo, H., Wang, G., Chen, X., \& Zhang, C. (2017). Towards Good Practices for Deep 3D Hand Pose Estimation. in \emph{arXiv preprint arXiv:1707.07248}.
\item Shrivastava, A., Pfister, T., Tuzel, O., Susskind, J., Wang, W., \& Webb, R. (2016). Learning from simulated and unsupervised images through adversarial training. in emph{arXiv preprint arXiv:1612.07828}.
\item Glorot, X., \& Bengio, Y. (2010, March). Understanding the difficulty of training deep feedforward neural networks. In \emph{ Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics (pp. 249-256)}.
\item Kumar, S. K. (2017). On weight initialization in deep neural networks. \emph{arXiv preprint arXiv:1704.08863}.
\item He, K., Zhang, X., Ren, S., \& Sun, J. (2015). Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In \emph{Proceedings of the IEEE international conference on computer vision (pp. 1026-1034)}.
\item Wan, C., Probst, T., Van Gool, L., \& Yao, A. (2017). Crossing Nets: Combining GANs and VAEs with a Shared Latent Space for Hand Pose Estimation. In \emph{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 680-689)}.
\end{enumerate}

\end{document}